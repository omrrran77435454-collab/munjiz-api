# summarizer.py

# ----------------------------------
# الخطوة 1: استيراد المكتبات اللازمة
# ----------------------------------
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import heapq  # مكتبة للمساعدة في إيجاد أكبر العناصر

# ----------------------------------
# الخطوة 2: النص الأصلي الذي نريد تلخيصه
# ----------------------------------
# يمكنك تغيير هذا النص بأي مقال طويل تريده
text = """
 ،ﺚﻳﺪﺤﻟا
 
ﺮﺼﻌﻟا
 
ﻲﻓ
 
ﺔﻴﺟﻮﻟﻮﻨﻜﺘﻟا
 
تﻻﻮﺤﺘﻟا
 
زﺮﺑأ
 
ﺪﺣأ
 (Artificial Intelligence - AI) ﻲﻋﺎﻨﻄﺻﻻا ءﺎﻛﺬﻟا 
ﻞﺜﻤﻳ
 ﻲﺘﻟا
 
مﺎﻬﻤﻟا
 
ﺬﻴﻔﻨﺗو
 
ةﺎﻛﺎﺤﻣ
 
ﻰﻠﻋ
 
ةردﺎﻗ
 
ﺞﻣاﺮﺑو
 
ﺔﻤﻈﻧأ
 
ﺮﻳﻮﻄﺗ
 
ﻰﻟإ
 
فﺪﻬﻳ
 
بﻮﺳﺎﺤﻟا
 
مﻮﻠﻋ
 
ﻦﻣ
 
مﺪﻘﺘﻣ
 
عﺮﻓ
 
ﻮﻫو
 ﻪﻓ ّﺮﻋو
 
،
 1956 
مﺎﻋ
 
ﻲﻓ
 
ﺢﻠﻄﺼﻤﻟا
 
اﺬﻫ
 
ﻲﺛرﺎﻜﻣ
 
نﻮﺟ
 
بﻮﺳﺎﺤﻟا
 
ﻢﻟﺎﻋ
 
غﺎﺻ
 
ﺪﻗو
 .
 1 
يﺮﺸﺒﻟا
 
ءﺎﻛﺬﻟا
 
ًةدﺎﻋ
 
ﺐﻠﻄﺘﺗ
 ﻞﻤﺸﺘﻟ
 
تﺎﻧﺎﻴﺒﻠﻟ
 
ﺔﻴﻟﻵا
 
ﺔﺠﻟﺎﻌﻤﻟا
 
دﺮﺠﻣ
 
ﺔﻤﻈﻧﻷا
 
هﺬﻫ
 
تارﺪﻗ
 
زوﺎﺠﺘﺗ
 .
 2 "
 ﺔﻴﻛﺬﻟا
 
تﻻﻵا
 
ﻊﻨﺻ
 
ﺔﺳﺪﻨﻫو
 
ﻢﻠﻋ
 " 
ﻪﻧﺄﺑ
 ﻢﻟ
 
ﻲﺘﻟا
 
ﻒﻗاﻮﻤﻟا
 
ﻲﻓ
 
ﻰﺘﺣ
 
،
 كاردﻹا
 و
 
،
 تاراﺮﻘﻟا ذﺎﺨﺗا
 و
 
،
 ﻲﻘﻄﻨﻤﻟا جﺎﺘﻨﺘﺳﻻا
 و
 
،
 ﻢﻠﻌﺘﻟا 
ﻞﺜﻣ
 
ةﺪﻘﻌﻣ
 
ﺺﺋﺎﺼﺧ
 . 1 
ﺢﻳﺮﺻ
 
ﻞﻜﺸﺑ
 
ﺎﻬﻴﻠﻋ
 
برﺪُ ﺗ
 
وأ
 
ﺞﻣﺮﺒُ ﺗ
"""

# ----------------------------------
# الخطوة 3: تنظيف النص وتقسيمه إلى جمل
# ----------------------------------
# تقسيم النص إلى جمل
sentences = sent_tokenize(text)

# ----------------------------------
# الخطوة 4: حساب تكرار الكلمات المهمة
# ----------------------------------
# الحصول على قائمة الكلمات غير المهمة باللغة الإنجليزية
stop_words = set(stopwords.words("english"))

# قاموس لتخزين تكرار كل كلمة
word_frequencies = {}

# المرور على كل كلمة في النص
for word in word_tokenize(text):
    # التأكد من أن الكلمة ليست من الكلمات غير المهمة
    if word.lower() not in stop_words:
        # التأكد من أنها كلمة (ليست علامة ترقيم)
        if word.isalpha():
            # زيادة عداد الكلمة
            if word not in word_frequencies:
                word_frequencies[word] = 1
            else:
                word_frequencies[word] += 1

# إيجاد الكلمة الأكثر تكرارًا
maximum_frequency = max(word_frequencies.values())

# توحيد أوزان الكلمات (بقسمة تكرار كل كلمة على تكرار الكلمة الأكثر شيوعًا)
for word in word_frequencies:
    word_frequencies[word] = (word_frequencies[word] / maximum_frequency)

# ----------------------------------
# الخطوة 5: تقييم الجمل
# ----------------------------------
# قاموس لتخزين درجة كل جملة
sentence_scores = {}

# المرور على كل جملة
for sent in sentences:
    # المرور على كل كلمة في الجملة
    for word in word_tokenize(sent.lower()):
        # إذا كانت الكلمة من الكلمات المهمة التي حسبناها
        if word in word_frequencies:
            # التأكد من أن طول الجملة مناسب (لتجنب الجمل القصيرة جدًا)
            if len(sent.split(' ')) < 30:
                # إضافة وزن الكلمة إلى درجة الجملة
                if sent not in sentence_scores:
                    sentence_scores[sent] = word_frequencies[word]
                else:
                    sentence_scores[sent] += word_frequencies[word]

# ----------------------------------
# الخطوة 6: الحصول على أفضل الجمل لإنشاء الملخص
# ----------------------------------
# تحديد عدد الجمل التي نريدها في الملخص (يمكنك تغيير هذا الرقم)
summary_length = 3 

# استخدام heapq للحصول على أفضل N جمل بناءً على درجاتها
summary_sentences = heapq.nlargest(summary_length, sentence_scores, key=sentence_scores.get)

# تجميع الجمل المختارة في نص واحد
summary = ' '.join(summary_sentences)

# ----------------------------------
# الخطوة 7: طباعة النتيجة
# ----------------------------------
print("==================== النص الأصلي ====================")
print(text)
print("\n==================== الملخص ====================")
print(summary)
